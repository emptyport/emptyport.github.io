{
  
    "blog-what-i-want-to-see-in-proteomics": {
      "title": "What I want to see in proteomics",
      "url": "/blog/what-i-want-to-see-in-proteomics/",
      "summary": "I love proteomics. I really do. It’s a great field full of fantastic scientists. But there are a few things that I think are holding the field back.The hardware is closed offIf you have a cool idea for something that you think you can get your instrument to do, you...",
      "content": "I love proteomics. I really do. It’s a great field full of fantastic scientists. But there are a few things that I think are holding the field back.The hardware is closed offIf you have a cool idea for something that you think you can get your instrument to do, you either have to pray you actually can come up with some method using the vendor’s software, give up, assemble your own instrument, or work in the lab of Matthias Mann. One of the recent papers that created a lot of buzz was about the boxcar method for collecting data. The Mann lab as of this writing still has not released their software that lets you run a boxcar method (reproducibility anyone?). I cobbled together some code that could create the boxcar methods, but my instrument was not reading the files. I was finally able to find out from the Mann lab that the boxcar method requires a custom license file from Thermo. Why do they get the custom license file but no one else does? How many other Mann labs are out there, but not recognized as such, because they don’t have this sort of control over their instruments? I think that the instrument vendors should give us scientists more control over the instruments. I think that access to the instrument’s API should be standard and included with the purchase of the instrument. If I had complete control over my instrument, I would be more likely to buy from that company in the future. To me it seems like a win-win situation.The hardware and software are siloed off from each otherHere is what I mean by that. Let’s say your lab gets a Lumos. You must use Xcalibur to create your instrument method and you must wait for your data to be collected and saved as a file before you can identify your peptides. If you want to use third party software to perform your search, you have to use software from Thermo to decode the raw file and convert it to an open format that the third party software can read. But now let’s imagine that the instrument vendors open up their hardware. Then these silos would naturally disappear (so I guess this problem is the same as the first problem I mention). If you could access the spectra as they are being collected through an API, then, instead of collecting your file and then searching it, you could be searching each spectrum as it is collected. As soon as your run finishes, you have your peptide IDs!Better file standardsEvery peptide search engine uses a different format. They are getting better at supporting pepXML and mzIdentML, but I don’t consider either of those formats to be suitable standards. I looked up the pepXML file specification and all I could find was an xsd file that defines the structure of the XML file. Good to have, but not a friendly way to learn the file structure. I looked up the mzIdentML file specification and I found a 91 page document. THAT IS TOO LONG!!! It looks as though the file format wants to be inclusive of all sorts of identification information, but in doing so it has become too complex. Rather than try to cater to all types of experiments, there should be a file format for peptide IDs, for protein IDs, for imaging data, etc. And while we are on the subject, I don’t think XML is a good choice. It does give you control over the structure of the file and you can nest information, but XML is verbose and adds a lot to the size of the file. I think JSON would be a better choice because it is a little less verbose, or alternatively just use a delimited file like a csv file. Everyone works with their ID information in table form anyway, so let’s just start with our data in that format.Consistency among search enginesI thought it would be a good idea to create a data type for working with peptide IDs. However, there is a lot of inconsistency in what information the peptide search engines give back to you. One useful piece of information that is often omitted is the retention time. I can go back to the spectrum file and look it up, but rather than have to look through potentially gigabytes of information, why not just include that information with the ID to begin with?Wasted computationEach time you run a search, you have to generate the theoretical peptides. I’ve recently learned that some search engines can save the theoretical peptides for use in future searches, but I think there ought to be a universal standard that all search engines can use. This would save quite a bit of time because rather than compute the peptides each time you run a search, all you have to do is pull them from a database and do the spectral matching."
    }
    ,
  
    "blog-my-programming-history": {
      "title": "My Programming History",
      "url": "/blog/my-programming-history/",
      "summary": "This post is somewhat similar to my last post about switching to JavaScript, but it covers my programming history in more general terms.I originally got into programming so that I could build my own laser light show. So far I’ve only managed to make a regular light show, but programming...",
      "content": "This post is somewhat similar to my last post about switching to JavaScript, but it covers my programming history in more general terms.I originally got into programming so that I could build my own laser light show. So far I’ve only managed to make a regular light show, but programming has stuck with me and has played a major role in my research and work.I got into bioinformatics early on in my programming career. I started out in Ruby and then made the switch to Python. As an undergraduate I developed the ISotope Extractor, or ISE, for Deuterater. Deuterater calculates how quickly proteins are created or destroyed using data from mass spectrometers. The problem with mass spectrometry data is that the files are rather large and contain a lot of noise. There were several iterations of ISE, but the one we finally settled on ended up being quite simple. The below video gives a demonstration of how it works. The basic idea is to find a set of peak series and condense them down into a single series of peaks. It’s also one of my first attempts at creating a 3d animation, so bear with me.After I graduated with my Bachelor’s degree, I worked at Qualtrics for a year and a half. I started out in customer support and worked my way into the role of a support engineer. I took care of bug fixes in existing PHP code, rewrote some custom APIs (also in PHP), and then designed and implemented an API from scratch in Ruby on Rails. By this point I’d forgotten most of my Ruby and this was my first time working with Rails, so I’m by no means an expert with Rails.Also while at Qualtrics, I started to get into web programming and machine learning. I wrote a couple small utility programs in Python using natural language processing. One program could analyze the contents of an email and determine if it was an automatic out of office response and the other was the beginnings of a chatbot. The out of office filter did not end up being implemented, but there was interest in the chatbot. A patent application was filed for the chatbot and that is currently pending.Since my time at Qualtrics, I’ve applied myself more to learning web development. I’ve completed the Responsive Web Design and Javascript Algorithms and Data Structures certifications from freeCodeCamp.Currently I dedicate a lot of my programming time on creating JavaScript modules for bioinformatics, in particular proteomics. I’ve released several modules on npm under an MIT license. They range from simple utility modules for parsing molecular formula to more specialized modules for simulating peptide fragmentation inside a mass spectrometer.One of my pet projects is Omics.js, a collection of JavaScript modules that are useful to the “omics.” Omics.js is my way of compiling a list of good, bioinformatics focused JavaScript modules for the community and to see what is still missing. I am slowly filling in the gaps with my own modules that you can see on my Github or on npm. In my experience, most scientific programming is done in Python. JavaScript is not the obvious choice for starting a bioinformatics project, but in my experience it is fast, easy to learn, and coupling JavaScript with HTML/CSS can provide for friendlier ways of viewing data. That being said I still do program in Python, but I am migrating to JavaScript."
    }
    ,
  
    "blog-why-i-m-switching-to-javascript": {
      "title": "Why I'm switching to JavaScript",
      "url": "/blog/why-i'm-switching-to-javascript/",
      "summary": "The first programming language I ever learned was Java. This was back in high school and I’ve since forgotten most of the syntax. I next became acquainted with Ruby when I joined a lab in college that used Ruby for most of their programming needs. The PI (principal investigator) was...",
      "content": "The first programming language I ever learned was Java. This was back in high school and I’ve since forgotten most of the syntax. I next became acquainted with Ruby when I joined a lab in college that used Ruby for most of their programming needs. The PI (principal investigator) was a self-taught programmer and he’d latched onto Ruby. He also happened to be the author of the mspire mass spectrometry library which he wrote in Ruby. Also during this time I took a class which used C++, but I did my best to stay away from it.My PI left while I still had about a year and a half left, and the lab I switched to was using Python. I made the switch from Ruby to Python and I was generally happy. Python also had a mass spectrometry library and a lot of support for numerical and scientific computing.After I graduated, I worked for a year and a half at Qualtrics. Part of my time there was spent providing support for some of the custom projects that were built for clients. While the newer projects were using languages like Python and Ruby, the older ones that I spent quite a bit of time maintaining were written in PHP. I’ve decided I don’t like PHP and am going to do my best to stay away. PHP can go hangout with C++.However, during my time at Qualtrics, I also started using JavaScript. I learned a little bit of Angular and a little bit of React, but I really enjoyed using Node. One of the things I like about Node is how the modules are installed locally only for your specific project unless you specify that they be installed on a global level. I know Python can use virtual environments and whatnot to do the same, but Node keeps everything isolated by default; I find it easier to use. Node modules also seemed to be more focused than the Python libraries I was using. Pyteomics is a great mass spec library, but it comes with a lot of functionality and is not documented super well. Trying to look through such a large code base to figure out how certain things work is certainly time consuming.JavaScript on the other hand is a blank slate. There are very few existing npm modules geared towards mass spec so I get to write them as modular as I like. I probably will combine all my modules into a larger library akin to Pyteomics or mspire, but I want to make sure each component is available individually and is well documented with test cases.Now if all I wanted was more modularity, I could just write my own modules in Python. That definitely is an option, but cloud computing is here to stay and I want to make sure there are libraries for mass spec that can be incorporated into web interfaces. Essentially any language can be used on the back-end, but only JavaScript can be used on the front-end. That is why I’ve created Omics-JS. Omics-JS is a place where you can find mass spec oriented modules and libraries. Most of what’s on there is stuff I’ve written, but there are also a few other libraries that others have written.My hope is to get more people excited about using JavaScript and to make it more mainstream in the mass spec community."
    }
    ,
  
    "blog-colored-protein-surfaces-blender": {
      "title": "Coloring Protein Surfaces in Blender",
      "url": "/blog/colored-protein-surfaces-blender/",
      "summary": "As I start my PhD and get back into biochemistry, I’m also getting back into animation. Animation is a great way to visual how proteins function because proteins are three dimensional molecules that interact in three dimensions. UCSF Chimera is an excellent tool for viwing protein structures and is capable...",
      "content": "As I start my PhD and get back into biochemistry, I’m also getting back into animation. Animation is a great way to visual how proteins function because proteins are three dimensional molecules that interact in three dimensions. UCSF Chimera is an excellent tool for viwing protein structures and is capable of producing animations. However, if you’ve ever seen the Inner Life of a Cell video, you’ll probably want to go for software that is more geared towards animation. My software of choice is Blender. Chimera is probably sufficient for the vast majority of small, simple animations, but you’ll definitely want to make a switch over to Blender if you plan on making a more complicated animation.One of the hurdles I ran into as I got started with Chimera and Blender was how to get a protein surface into Blender and keep the color information. This isn’t a problem if the protein molecule is going to be a single color, but if you want to show any sort of surface coloring such as hydrophobicity or electrostatic potential, you need to be sure to export that information from Chimera and be able to use it in Blender. Fortunately, the process is quite simple.For this example, I am using the Scorpion toxin BJXTR-IT protein structure from PDB. Your first order of business will be to download the .pdb file and open it in Chimera.Once I opened my structure in Chimera, I went to the Actions menu and hid the ribbon structure because I want to show the protein surface. Technically you can leave the ribbon structure because it will be hidden under the surface, but when you import the file into Blender you are going to end up with both the ribbon and the surface. After hiding the ribbon structure, I went back to the Actions menu and showed the surface.You can color your protein surface however you desire, but for this example I am going to go to the Tools menu and select Coulombic Surface Coloring from the Surface/Binding Analysis submenu. The Coulombic surface coloring will color your protein surface according to its electrostatic potential. There are more accurate tools for doing this, but this option does a pretty good job and is quick and easy to use.Now that I’ve colored my protein surface, I am going to open up the File menu and then select the Export Scene option. Choose a filename and location and then select X3D as the filetype. Click on save and then go ahead and open up Blender.Once you are in Blender, go ahead and delete the cube from the default scene. I’m going to go ahead and leave the camera and lamp. Now go to File, Import, and then select X3D Extensible 3D and then find the file you just saved from Chimera. This will import a rather large protein surface and possibly some lights. Go ahead and delete the extra lights, scale down your protein, and select smooth shading. My protein’s origin is also offset so I also set the geometry to origin.Go ahead and give your scene a quick render. If you don’t see any color on the protein surface, not to worry, we are now going to take care of that. The process is a little different depending on if you’re using the internal renderer or Cycles, so I’ll cover both.Blender InternalIf you plan on using the internal renderer, open up the Materials for your protein. Go down to the options and make sure the “Vertex Color Paint” option is checked. Now re-render your protein and you should see some color!CyclesIf you plan on using Cycles, open up the Node editor and create a node for your protein’s material. Create a new node and select “Attribute” from the Input submenu. In the name of the Attribute node put “Col” and connect its color to the color of the material node you are using. Give things a render to make sure the colors are showing up and you are good to go!Go ahead and setup your scene how you’d like and make something cool!"
    }
    ,
  
    "blog-lab-shakers": {
      "title": "Overpriced Lab Shakers",
      "url": "/blog/lab-shakers/",
      "summary": "While I was an undergrad, one of my research projects dealt with scorpion fluorescence. One of my tasks was to extract the fluorophores from the cuticle. I won’t go into any details about the protocol, but I ended up with ground up scorpion cuticle in ethanol. At this point we...",
      "content": "While I was an undergrad, one of my research projects dealt with scorpion fluorescence. One of my tasks was to extract the fluorophores from the cuticle. I won’t go into any details about the protocol, but I ended up with ground up scorpion cuticle in ethanol. At this point we were wanting to stir our scorpion slurry so that all the bits and pieces would be exposed to the ethanol so that our fluorophores could dissolve. Unfortunately, we were borrowing lab space for our project and did not have access to a lab shaker. We did have a small budget to work with so I looked up lab shakers online. As soon as I saw the prices, ranging from several hundred to several thousand dollars, I knew there had to be a cheaper way to keep my samples stirring.A lab shaker in its most basic form is a platform that rocks back and forth. There are some variations on this idea such as orbital shakers, and you can also get shakers with fancy speed and temperature controls, but for our purposes we just needed a rocking platform. Why such a device would cost so much is beyond me. My best guess is that when researchers are buying equipment, they are usually sitting on a pile of money (which actually probably isn’t enough money to be called a ‘pile’) in the form of start-up funds or a grant. A lab shaker is one of those ubiquitous pieces of lab equipment that you just need to get, and when you look at the price of a lab shaker in the context of a grant, it doesn’t seem quite as expensive. I really don’t think lab shakers should be that expensive, but they are. My solution to the problem was to make one myself.While I didn’t really document what I did, the following video provides an overview of how my lab shaker turned out.The best part of my lab shaker? I built it using trash and parts I already had. Essentially it would turn the CD drive motor on for a short amount of time to push the tray out and then the rubber band would pull the CD tray back down. It’s not elegant and I had to change out the rubber band every so often, but it worked.If you are interested in making your own lab shaker, there are some instructions on Instructables for a shaker similar to mine that goes back and forth using a CD drive. If you are feeling fancy, you can build your own orbital shaker, but that will cost a bit more.Lab equipment tends to be quite expensive, and some of it rightfully so, but general purpose lab shakers should not be as expensive as they are."
    }
    ,
  
    "blog-what-in-the-world-is-proteomics": {
      "title": "Proteomics? What in the world is that!?",
      "url": "/blog/what-in-the-world-is-proteomics/",
      "summary": "One of the fields I am interested in is that of proteomics. The problem with that is that the average person probably has never heard that word in their life, so I usually tell people that I study proteins. While this certainly avoids a lot of confusion, saying that I...",
      "content": "One of the fields I am interested in is that of proteomics. The problem with that is that the average person probably has never heard that word in their life, so I usually tell people that I study proteins. While this certainly avoids a lot of confusion, saying that I study proteins is still a pretty vague answer. This post is designed to be an introduction to proteomics that anyone (including your grandma) can understand.So what is proteomics? Proteomics is a cousin to genomics, which is something you may have heard of before. Genomics deals with figuring out what the genetic code of somebody/something is. Essentially genomics is figuring out what DNA is in your body. Your genetic code acts as the blueprint that all your body’s cells use when making proteins. If you think of the DNA as the instructions to a Lego set, you can think of the proteins as the Lego creation itself. The DNA dictates which proteins should be made. Proteomics deals with figuring out what proteins are actually being made in your body. This sounds like a fairly simple task, but each organ in your body makes varying amounts of protein, and cells will often make special modifications to proteins. The goal of proteomics is to decode all that information so that we have a better understanding of how cells work and so that we can develop better drugs to fight disease.One of the tools that makes proteomics possible is the mass spectrometer, or mass spec for short. A spectrometer is something used to measure data, and in the case of a mass spec, we are measuring mass. Why measure mass? Knowing how much a protein weighs goes a long way in figuring out what the protein is. We can also perform some neat tricks with the mass that help us figure out what all the individual parts of the protein are. You could almost say we are working with a glorified bathroom scale, but let’s remember that we are talking about measuring proteins, so how we measure their mass is actually a bit different.To measure the mass of proteins, the mass spectrometer will use magnetic and electric fields to throw the proteins at a detector. It’s sort of like if you decided to weigh all your kids’ Lego sets by launching them across your lawn with a catapult. If we know with how much force the catapult is launching the Legos, we can use the distance that each item travels to measure its mass. A mini Lego creation will travel much farther than a Lego house. This is how the mass spec works. The further down the detector a protein hits, the lighter it is; the closer a protein hits, the heavier it is. We can then calculate the mass of the protein since we know how hard the mass spectrometer is throwing the proteins.However, there is a problem with this method. Proteins can often come in very similar, if not identical, masses. To further obfuscate things, proteins come in an extreme range of sizes and consequently weights. If we wanted to measure the mass of a life-size Lego replica of Batman with the catapult, the mini Lego creation would end up in the neighbors yard! On the other hand, if we decreased the strength of the catapult so that the mini Lego creation stayed in our yard, we could then no longer measure the mass of life-size Batman because it wouldn’t go anywhere. To get around this problem, scientists will actually chop the proteins up into smaller pieces.To chop the proteins up into smaller pieces, scientists use a special protein called an enzyme that will chop other proteins at specific locations. The resulting pieces, called peptides, still vary in their mass, but the variation is much smaller. Rather than dealing with a range of mini Lego creation to life-size Batman, we are now dealing with a range of mini Lego creation to Lego house. However, we now have another problem. Rather than being able to weigh each Lego set, we are now only weighing chunks of our Lego sets. Unfortunately, the chunks of our Lego sets are now mixed up with each other! To further add to the confusion, there are now so many chunks that they are piling up all over the yard! It would look like we are worse off than before when it comes to using a mass spectrometer, but us sneaky chemists still have a few tricks up our sleeves.Let’s first examine the problem of everything now overlapping since we have so many chunks. An important method used by chemists is chromatography. This is sort of an awkward word, but essentially chromatography means we are separating out chemicals. I’m going to use an analogy to explain how chromatography works. Let’s say you are the principal of a school and you want to help prevent overcrowding when school lets out. To do so, you are going to let the kindergarteners leave first, and then move on up through the grades until you get to the sixth graders. As each class gets out, you’ll see a trickle of students coming out the doors (the kids who just can’t wait to get out of school), then you’ll see the main body of students coming out, and then you’ll see a few stragglers. Then you’ll see the same process repeat with the next grade up. This is exactly how chromatography works. A chemist will put all their chemicals on one side of a column, and then they’ll control the conditions that dictate what gets to move through the column. The result is that their chemicals come out the other end of the column one-by-one. With proteomics, we are usually dealing with so many different peptides that we can’t separate them one-by-one with chromatography, but we can separate them enough so that only a few come out at a time. It’s okay if the kindergarteners come out with the fourth graders because we can still see that they are different ages; likewise we don’t mind if several peptides are analyzed at the same time because they most likely have different masses. If we return to our Lego analogy, we are now grabbing small bucketfuls of Lego chunks from our pile of Lego chunks, catapulting and measuring those chunks, cleaning up the yard, and then repeating with the next bucketful. The process is much slower, but we are making sure we remain in the bounds of our yard, and we avoid the problem of trying to measure too much at once.Okay, so we’ve solved the problem of too much stuff at once, but now what about the problem of not knowing what Lego chunk goes with what set? We chemists once again have another trick up our sleeves. You see, since we know what the genetic code of humans is, we know what proteins humans theoretically can make. Since we know what proteins can be found in humans, we don’t really care that we broke our proteins up into smaller pieces and then mixed them together. If we have the DNA for our Lego sets, AKA the instruction manuals, mixing up our Lego chunks isn’t the end of the world because we can pick up any given piece and figure out which Lego set it belongs to. A particular Lego piece might be shared between two Lego sets, but since Lego sets often come with unique pieces, we can still figure out which Lego sets we have in our pile of Legos. It’s the same thing with the peptides. We are looking to see what peptides we have and then we are matching them back to the potential proteins that a human can make.The last hurdle we need to clear is how we can use mass to figure out what a certain peptide (or Lego chunk) is. The neat thing about a mass spectrometer is that once it finds a peptide, it can then smash that peptide and look at those pieces. How does that work? The building blocks of proteins (and peptides) are called amino acids. They are sort of like Lego pieces. When the mass spectrometer smashes a peptide, it will break the peptide in between the amino acids. If you bought one hundred Lego houses and then threw them against a wall, you would end up with lots of different sized chunks. If you arranged these chunks from smallest to biggest (or lightest to heaviest in the case of peptides), the gaps in between will tell you what piece is there. For example, let’s say I have a chunk of the Lego house with a door, and the same chunk of the house but without the door. If I were to weigh each chunk, the difference between the two chunks would be the weight of the door. When the mass spectrometer breaks the peptides, we use those gaps to figure out the sequence of the peptide. That sequence is then what we use to match the peptide back to a protein.I know we just covered a lot of information and we used a lot of analogies, so let’s summarize everything.  We have a collection of Lego sets that we want to identify.  We break those Lego sets up into roughly equal chunks (but we are not breaking them down to individual pieces).  We catapult the chunks across our yard and measure how far each chunk went. This gives us the mass of each chunk.  Next we take each collection of chunks and we smash them against a wall.  We organize the smashed pieces from smallest to largest.  The differences between masses will tell us which Lego pieces we have in each chunk.  We take our list of pieces and go through the instruction manuals until we find which set(s) those pieces came from.The analogy isn’t perfect because we can see the Lego sets with our eyes and figure things out that way, but with proteins we aren’t able to see them.If you are interested in a more technical introduction to proteomics, the Wikipedia page on the subject gives a brief overview.If you’ve made it to the end of this post, I want to thank you for reading and I hope your understanding of proteomics is a little better than it was before you got here."
    }
    ,
  
    "blog-bias-in-publishing": {
      "title": "Should papers that are clearly biased be published?",
      "url": "/blog/bias-in-publishing/",
      "summary": "You may have heard the recent news about how playing American football leads to brain injury. At least that’s what the headlines are saying. Headlines such as the click-baity “111 N.F.L. Brains. All But One Had C.T.E.” from the New York Times and the apocalyptic “The CTE Study That Could...",
      "content": "You may have heard the recent news about how playing American football leads to brain injury. At least that’s what the headlines are saying. Headlines such as the click-baity “111 N.F.L. Brains. All But One Had C.T.E.” from the New York Times and the apocalyptic “The CTE Study That Could Kill Football” from Forbes. But if you read the articles, you’ll quickly learn that the study only includes brains that were donated to a program studying repetitive head trauma. If you take a look at Table 3 in the original article, it’s pretty clear that the majority of the individuals whose brains were donated exhibited signs of cognitive decline. In other words, these brains were coming from individuals who were suspected of having possible brain damage. In the words of the article’s primary author, Dr. Ann McKee,  “Families don’t donate brains of their loved one unless they’re concerned about the person. So all the players in this study had - on some level, were symptomatic. So that leaves you with a very skewed representation.”The headlines coming from NPR, CNN, and ESPN were all worded in such a way as to imply that the study was limited in scope.While the news articles varied in their headlines, at least they all mentioned the bias involved in the study in the body of the story. Unfortunately, however, these disclaimers don’t show up until after the article gives a summary of the research. If someone doesn’t finish reading the article, they are going to miss the disclaimer and go about their day believing this to be a comprehensive study. In contrast to the news reports, the original article makes it crystal clear from the beginning that they were not able to conduct their study on a representative population.So all this brings me to my question.  “Should papers that are clearly biased be published?”Let’s first examine why they should be published. Even though this article is limited to only those brains which were donated on suspicion of brain damage, the fact that the majority did have brain damage is convincing evidence that this is an area worthy of research. Whether or not football is to blame for the brain damage remains to be seen, but this study hints that such may be the case.Why shouldn’t biased articles be published? I think an obvious reason would be the headlines that this particular article generated. This study is being used as click-bait and as a sensational headline. There is a big push for people to be educated, and maybe the sites using this article as click-bait will say that is what they are doing, but it is clear that the results of this study are being exaggerated when you factor in the bias. But how the article is portrayed in the media is out of the hands of the authors. Even a perfectly designed and executed study could be used in such a way. On a more scientific note, it is hard to say whether or not the results of this study are meaningful due to the bias. The prevalence of brain damage might not actually be higher in NFL players than in the rest of the population, or it could be due to some other factor besides football. We just can’t say with the data that’s been collected.If news companies were better with their headlines and with how they summarize research articles, I don’t think it would be too big of a deal if an article is published which discloses any potential bias. Those involved with the study did make a finding and it is a good starting point for future research. There are some fields where bias is unavoidable, and I think research involving human brains is definitely one of those fields. It is always better to avoid any sort of bias in your research, but recognizing your sources of bias is the next best thing. Unfortunately, scientific articles are often difficult even for scientists to read and understand, and to further add to the issue, articles are often locked behind paywalls. Since these articles can be inaccessible to the average person, we rely on news sources to distill their findings into easy to understand news articles.So what is one to do? Since eliminating bias completely isn’t feasible, my recommendation is to always try to find the original scientific article and read it critically to make sure you are understanding what is actually going on. If you are unable to do that, try to find the same story from a few different sources to try to “average” things out."
    }
    ,
  
    "blog-linux-at-ut-southwestern": {
      "title": "Using Linux at UT Southwestern",
      "url": "/blog/linux-at-ut-southwestern/",
      "summary": "I recently made a complete switch to Linux. I’d been dual booting Windows 10 and Ubuntu before, but I decided to start fresh before I started my PhD program. I am now running solely elementary OS (no more Windows) and I have to say I am quite pleased so far.However,...",
      "content": "I recently made a complete switch to Linux. I’d been dual booting Windows 10 and Ubuntu before, but I decided to start fresh before I started my PhD program. I am now running solely elementary OS (no more Windows) and I have to say I am quite pleased so far.However, there was a small hiccup with my transition to 100% Linux and that has to do with my school’s wifi. Since I am a student, I have to sign in to their wifi with my student credentials and use a proxy server if I want to connect outside of UT Southwestern’s intranet. I followed their instructions for Android (so I could get the proxy host and port) and I had no problem connecting to outside websites. Unfortunately, if I wanted to install anything with apt-get or the app center, my laptop couldn’t connect to the software repositories. I wouldn’t say installing new programs is something I do routinely, but since this is a fresh install I have been installing quite a few applications lately.For a few days I just put up with it and waited until I got home to install new software, but I got sick of waiting until I got home so I decided to just go ahead and figure out how to make apt work with the proxy. After a little googling I discovered a simple solution.To solve my connection issue, I created the following file /etc/apt/apt.conf and added the following lines to it:Acquire::http::proxy &quot;http://user:pass@{server}:{port}&quot;;Acquire::https::proxy &quot;http://user:pass@{server}:{port}&quot;;The {server} and {port} have been replaced by my school’s proxy servers host and port. Since my school username and password are required as part of connecting to the wifi, I did not have to place those in the file; the lines really do read “user:pass” before the @ symbol. If I had needed to put my actual username and password here for it to work, I’m not sure that I would have done it since this file is just stored as plain text.Hopefully this simple solution will be useful to anyone using Linux in a Windows and macOS world.–Edit–Once you disconnect from the proxy (like when you get back home), you have to comment out the two lines in /etc/apt/apt.conf with a #. Otherwise, apt and the app center will try to connect through a proxy that is no longer there."
    }
    ,
  
    "blog-welcome": {
      "title": "Welcome!",
      "url": "/blog/welcome/",
      "summary": "This blog is where I’ll be sharing the occasional tutorial, interesting finding, project, or rant about something unimportant. For now it’s pretty empty, but be sure to check back in the future!",
      "content": "This blog is where I’ll be sharing the occasional tutorial, interesting finding, project, or rant about something unimportant. For now it’s pretty empty, but be sure to check back in the future!"
    }
    
  
}